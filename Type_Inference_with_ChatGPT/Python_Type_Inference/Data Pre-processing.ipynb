{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_text(post_id):\n",
    "    # Construct the post URL\n",
    "    url = f\"https://stackoverflow.com/questions/{post_id}\"\n",
    "\n",
    "    # Make the HTTP request and get the HTML content\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the title element\n",
    "    title_element = soup.find('h1', {'class': 'fs-headline1'})\n",
    "    title = title_element.get_text().strip()\n",
    "\n",
    "    # Find the post body element\n",
    "    post_body_elements = soup.find_all('div', {'class': 'js-post-body'})\n",
    "\n",
    "    # Extract the post body text for each answer element and preprocess\n",
    "    post_bodies = []\n",
    "    for post_body_element in post_body_elements:\n",
    "#         # Remove \"s-code-block\" class\n",
    "#         for s_code_block in post_body_element.select('pre'):\n",
    "#             s_code_block.extract()\n",
    "\n",
    "#         # Remove HTML tags\n",
    "#         for tag in post_body_element.find_all():\n",
    "#             tag.unwrap()\n",
    "\n",
    "        # Extract the cleaned up post body text\n",
    "        body_element = post_body_element.get_text()\n",
    "\n",
    "        post_bodies.append(body_element)\n",
    "\n",
    "    post_body = ''.join(post_bodies)\n",
    "    text = title + \"\\n\" + re.sub(r'\\n{2,}', ' ', post_body)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('/home/azmain/experiment.csv')\n",
    "\n",
    "output_folder = '/home/azmain/api_extractor_all_json'\n",
    "\n",
    "# Initialize a count variable\n",
    "count = 0\n",
    "\n",
    "# Loop through each row in the DataFrame and create a JSON file for it\n",
    "for index, row in df.iterrows():\n",
    "    time.sleep(10)\n",
    "    # Extract the values from the relevant columns\n",
    "    post_id = int(row['Post ID'])\n",
    "    api = row['API']\n",
    "    expected_output = row['Expected Output']\n",
    "    \n",
    "    post_text = str(get_text(post_id))\n",
    "    \n",
    "    # Create a dictionary to represent the JSON object\n",
    "    data = {\n",
    "        'post_id': post_id,\n",
    "        'api': api,\n",
    "        'expected_output': expected_output,\n",
    "        'text': post_text\n",
    "    }\n",
    "    \n",
    "    # Increment the count variable\n",
    "    count += 1\n",
    "    \n",
    "    # Construct the file path for the JSON file\n",
    "    file_path = os.path.join(output_folder, f'{count}_{post_id}.json')\n",
    "    \n",
    "    # Write the dictionary to a JSON file\n",
    "    with open(file_path, 'w') as outfile:\n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azmain_py39",
   "language": "python",
   "name": "azmain_py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
